{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1USazor4NVz1"
      },
      "source": [
        "# CleanRL's baseline setup for envpool using PufferLib install"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcdWwnRn0tAr"
      },
      "source": [
        "# Install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ofmqUcYp2ft1"
      },
      "outputs": [],
      "source": [
        "# Don't restart runtime until both finish (it'll ask after the first line)\n",
        "!pip install git+https://github.com/pufferai/pufferlib.git@1.0\n",
        "!pip install gymnasium[atari,accept-rom-license]==0.29.1 tensorboard==2.11.2 stable_baselines3==2.1.0 torch wandb envpool kron-torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWuSJPcM0q0s"
      },
      "source": [
        "# wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGGBTEmS0hg1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "wandb_key = getpass(\"Enter your Wandb API key: \")\n",
        "os.environ['WANDB_API_KEY'] = wandb_key\n",
        "\n",
        "print(\"Wandb API key has been set as an environment variable.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1-BpeuS0oqZ"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Y6RAHL0rqso"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "from collections import deque\n",
        "from IPython.display import display, Image\n",
        "import matplotlib.pyplot as plt\n",
        "import zipfile\n",
        "\n",
        "import envpool\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions.categorical import Categorical\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from kron_torch import Kron\n",
        "from kron_torch.kron import precond_update_prob_schedule\n",
        "\n",
        "\n",
        "class RecordEpisodeStatistics(gym.Wrapper):\n",
        "    def __init__(self, env, deque_size=100):\n",
        "        super().__init__(env)\n",
        "        self.num_envs = getattr(env, \"num_envs\", 1)\n",
        "        self.episode_returns = None\n",
        "        self.episode_lengths = None\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        observations = super().reset(**kwargs)\n",
        "        self.episode_returns = np.zeros(self.num_envs, dtype=np.float32)\n",
        "        self.episode_lengths = np.zeros(self.num_envs, dtype=np.int32)\n",
        "        self.lives = np.zeros(self.num_envs, dtype=np.int32)\n",
        "        self.returned_episode_returns = np.zeros(self.num_envs, dtype=np.float32)\n",
        "        self.returned_episode_lengths = np.zeros(self.num_envs, dtype=np.int32)\n",
        "        return observations\n",
        "\n",
        "    def step(self, action):\n",
        "        observations, rewards, dones, infos = super().step(action)\n",
        "        self.episode_returns += infos[\"reward\"]\n",
        "        self.episode_lengths += 1\n",
        "        self.returned_episode_returns[:] = self.episode_returns\n",
        "        self.returned_episode_lengths[:] = self.episode_lengths\n",
        "        self.episode_returns *= 1 - infos[\"terminated\"]\n",
        "        self.episode_lengths *= 1 - infos[\"terminated\"]\n",
        "        infos[\"r\"] = self.returned_episode_returns\n",
        "        infos[\"l\"] = self.returned_episode_lengths\n",
        "        return (observations, rewards, dones, infos)\n",
        "\n",
        "\n",
        "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
        "    torch.nn.init.orthogonal_(layer.weight, std)\n",
        "    torch.nn.init.constant_(layer.bias, bias_const)\n",
        "    return layer\n",
        "\n",
        "\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, envs):\n",
        "        super().__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            layer_init(nn.Conv2d(4, 32, 8, stride=4)),\n",
        "            nn.ReLU(),\n",
        "            layer_init(nn.Conv2d(32, 64, 4, stride=2)),\n",
        "            nn.ReLU(),\n",
        "            layer_init(nn.Conv2d(64, 64, 3, stride=1)),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            layer_init(nn.Linear(64 * 7 * 7, 512)),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.actor = layer_init(nn.Linear(512, envs.single_action_space.n), std=0.01)\n",
        "        self.critic = layer_init(nn.Linear(512, 1), std=1)\n",
        "\n",
        "    def get_value(self, x):\n",
        "        return self.critic(self.network(x / 255.0))\n",
        "\n",
        "    def get_action_and_value(self, x, action=None):\n",
        "        hidden = self.network(x / 255.0)\n",
        "        logits = self.actor(hidden)\n",
        "        probs = Categorical(logits=logits)\n",
        "        if action is None:\n",
        "            action = probs.sample()\n",
        "        return action, probs.log_prob(action), probs.entropy(), self.critic(hidden)\n",
        "\n",
        "\n",
        "def print_model_summary(model):\n",
        "    def print_layer_info(module, name=''):\n",
        "        if hasattr(module, 'weight'):\n",
        "            print(f\"{name}: {module.__class__.__name__}, \"\n",
        "                  f\"Weight shape: {tuple(module.weight.shape)}\")\n",
        "            if hasattr(module, 'bias') and module.bias is not None:\n",
        "                print(f\"  Bias shape: {tuple(module.bias.shape)}\")\n",
        "        elif isinstance(module, (nn.Flatten, nn.ReLU)):\n",
        "            print(f\"{name}: {module.__class__.__name__}\")\n",
        "\n",
        "    print(\"Model Summary:\")\n",
        "    for name, layer in model.named_children():\n",
        "        if isinstance(layer, nn.Sequential):\n",
        "            for i, sublayer in enumerate(layer):\n",
        "                print_layer_info(sublayer, f\"{name}.{i}\")\n",
        "        else:\n",
        "            print_layer_info(layer, name)\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"\\nTotal trainable parameters: {total_params}\")\n",
        "\n",
        "\n",
        "def main(\n",
        "    exp_name: str = \"ppo_atari\",\n",
        "    seed: int = 1,\n",
        "    torch_deterministic: bool = True,\n",
        "    cuda: bool = True,\n",
        "    track: bool = True,\n",
        "    wandb_project_name: str = \"cleanRL\",\n",
        "    wandb_entity: str = None,\n",
        "    env_id: str = \"Breakout-v5\",\n",
        "    total_timesteps: int = 10000000,\n",
        "    optimizer: str = \"adam\",\n",
        "    learning_rate: float = 2.5e-4,\n",
        "    weight_decay: float = 0.0,\n",
        "    num_envs: int = 8,\n",
        "    num_steps: int = 128,\n",
        "    anneal_lr: bool = True,\n",
        "    gamma: float = 0.99,\n",
        "    gae_lambda: float = 0.95,\n",
        "    num_minibatches: int = 4,\n",
        "    update_epochs: int = 4,\n",
        "    norm_adv: bool = True,\n",
        "    clip_coef: float = 0.1,\n",
        "    clip_vloss: bool = True,\n",
        "    ent_coef: float = 0.01,\n",
        "    vf_coef: float = 0.5,\n",
        "    max_grad_norm: float = 0.5,\n",
        "    target_kl: float = None,\n",
        "):\n",
        "    batch_size: int = int(num_envs * num_steps)\n",
        "    minibatch_size: int = int(batch_size // num_minibatches)\n",
        "    num_iterations: int = total_timesteps // batch_size\n",
        "    run_name = f\"{env_id}__{exp_name}__{seed}__{int(time.time())}\"\n",
        "    if track:\n",
        "        import wandb\n",
        "\n",
        "        wandb.init(\n",
        "            project=wandb_project_name,\n",
        "            entity=wandb_entity,\n",
        "            sync_tensorboard=True,\n",
        "            config={\n",
        "                \"exp_name\": exp_name,\n",
        "                \"seed\": seed,\n",
        "                \"torch_deterministic\": torch_deterministic,\n",
        "                \"cuda\": cuda,\n",
        "                \"track\": track,\n",
        "                \"wandb_project_name\": wandb_project_name,\n",
        "                \"wandb_entity\": wandb_entity,\n",
        "                \"env_id\": env_id,\n",
        "                \"total_timesteps\": total_timesteps,\n",
        "                \"optimizer\": optimizer,\n",
        "                \"learning_rate\": learning_rate,\n",
        "                \"weight_decay\": weight_decay,\n",
        "                \"num_envs\": num_envs,\n",
        "                \"num_steps\": num_steps,\n",
        "                \"anneal_lr\": anneal_lr,\n",
        "                \"gamma\": gamma,\n",
        "                \"gae_lambda\": gae_lambda,\n",
        "                \"num_minibatches\": num_minibatches,\n",
        "                \"update_epochs\": update_epochs,\n",
        "                \"norm_adv\": norm_adv,\n",
        "                \"clip_coef\": clip_coef,\n",
        "                \"clip_vloss\": clip_vloss,\n",
        "                \"ent_coef\": ent_coef,\n",
        "                \"vf_coef\": vf_coef,\n",
        "                \"max_grad_norm\": max_grad_norm,\n",
        "                \"target_kl\": target_kl,\n",
        "            },\n",
        "            name=run_name,\n",
        "            monitor_gym=True,\n",
        "            save_code=True,\n",
        "        )\n",
        "    writer = SummaryWriter(f\"runs/{run_name}\")\n",
        "    writer.add_text(\n",
        "        \"hyperparameters\",\n",
        "        \"|param|value|\\n|-|-|\\n%s\"\n",
        "        % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in locals().items()])),\n",
        "    )\n",
        "\n",
        "    # TRY NOT TO MODIFY: seeding\n",
        "    def set_seed_everywhere(seed):\n",
        "        random.seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        torch.manual_seed(seed)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    set_seed_everywhere(seed)\n",
        "\n",
        "    torch.backends.cudnn.deterministic = torch_deterministic\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() and cuda else \"cpu\")\n",
        "\n",
        "    # env setup\n",
        "    envs = envpool.make(\n",
        "        env_id,\n",
        "        env_type=\"gym\",\n",
        "        num_envs=num_envs,\n",
        "        episodic_life=True,\n",
        "        reward_clip=True,\n",
        "        seed=seed,\n",
        "    )\n",
        "    envs.num_envs = num_envs\n",
        "    envs.single_action_space = envs.action_space\n",
        "    envs.single_observation_space = envs.observation_space\n",
        "    envs = RecordEpisodeStatistics(envs)\n",
        "    assert isinstance(\n",
        "        envs.action_space, gym.spaces.Discrete\n",
        "    ), \"only discrete action space is supported\"\n",
        "\n",
        "    agent = Agent(envs).to(device)\n",
        "    agent.train()\n",
        "\n",
        "    # Print model summary\n",
        "    print_model_summary(agent)\n",
        "\n",
        "    agent = torch.compile(agent)\n",
        "\n",
        "    ########## optimizers ##########\n",
        "    if optimizer == \"adam\":\n",
        "        optimizer = optim.AdamW(\n",
        "            agent.parameters(), lr=learning_rate, weight_decay=weight_decay, eps=1e-5\n",
        "        )  # cleanrl baseline\n",
        "    elif optimizer == \"kron\":\n",
        "        optimizer = Kron(\n",
        "            agent.parameters(),\n",
        "            lr=learning_rate,\n",
        "            weight_decay=weight_decay,\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid optimizer: {optimizer}\")\n",
        "    ################################\n",
        "\n",
        "    # ALGO Logic: Storage setup\n",
        "    obs = torch.zeros((num_steps, num_envs) + envs.single_observation_space.shape).to(\n",
        "        device\n",
        "    )\n",
        "    actions = torch.zeros((num_steps, num_envs) + envs.single_action_space.shape).to(\n",
        "        device\n",
        "    )\n",
        "    logprobs = torch.zeros((num_steps, num_envs)).to(device)\n",
        "    rewards = torch.zeros((num_steps, num_envs)).to(device)\n",
        "    dones = torch.zeros((num_steps, num_envs)).to(device)\n",
        "    values = torch.zeros((num_steps, num_envs)).to(device)\n",
        "\n",
        "    # Print shapes of batch tensors\n",
        "    print(\"Batch tensor shapes:\")\n",
        "    print(f\"obs: {obs.shape}\")\n",
        "    print(f\"actions: {actions.shape}\")\n",
        "    print(f\"logprobs: {logprobs.shape}\")\n",
        "    print(f\"rewards: {rewards.shape}\")\n",
        "    print(f\"dones: {dones.shape}\")\n",
        "    print(f\"values: {values.shape}\")\n",
        "\n",
        "    avg_returns = deque(maxlen=20)\n",
        "    collected_rewards = []\n",
        "\n",
        "    # TRY NOT TO MODIFY: start the game\n",
        "    global_step = 0\n",
        "    start_time = time.time()\n",
        "    next_obs = torch.Tensor(envs.reset()).to(device)\n",
        "    next_done = torch.zeros(num_envs).to(device)\n",
        "\n",
        "    for iteration in range(1, num_iterations + 1):\n",
        "        # Annealing the rate if instructed to do so.\n",
        "        if anneal_lr:\n",
        "            frac = 1.0 - (iteration - 1) / num_iterations\n",
        "            lrnow = frac * learning_rate\n",
        "            optimizer.param_groups[0][\"lr\"] = lrnow\n",
        "\n",
        "        agent.eval()\n",
        "        for step in range(0, num_steps):\n",
        "            global_step += num_envs\n",
        "            obs[step] = next_obs\n",
        "            dones[step] = next_done\n",
        "\n",
        "            # ALGO LOGIC: action logic\n",
        "            with torch.no_grad():\n",
        "                action, logprob, _, value = agent.get_action_and_value(next_obs)\n",
        "                values[step] = value.flatten()\n",
        "            actions[step] = action\n",
        "            logprobs[step] = logprob\n",
        "\n",
        "            # TRY NOT TO MODIFY: execute the game and log data.\n",
        "            next_obs, reward, next_done, info = envs.step(action.cpu().numpy())\n",
        "            rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
        "            next_obs, next_done = (\n",
        "                torch.from_numpy(next_obs).to(device),\n",
        "                torch.from_numpy(next_done).to(device).float(),\n",
        "            )\n",
        "\n",
        "            for idx, d in enumerate(next_done):\n",
        "                if d and info[\"lives\"][idx] == 0:\n",
        "                    if iteration % 25 == 0:\n",
        "                        print(\n",
        "                            f\"global_step={global_step}, episodic_return={info['r'][idx]}\"\n",
        "                        )\n",
        "                    avg_returns.append(info[\"r\"][idx])\n",
        "                    writer.add_scalar(\n",
        "                        \"charts/avg_episodic_return\",\n",
        "                        np.average(avg_returns),\n",
        "                        global_step,\n",
        "                    )\n",
        "                    collected_rewards.append((global_step, np.average(avg_returns)))\n",
        "                    writer.add_scalar(\n",
        "                        \"charts/episodic_return\", info[\"r\"][idx], global_step\n",
        "                    )\n",
        "                    writer.add_scalar(\n",
        "                        \"charts/episodic_length\", info[\"l\"][idx], global_step\n",
        "                    )\n",
        "\n",
        "        agent.train()\n",
        "\n",
        "        # bootstrap value if not done\n",
        "        with torch.no_grad():\n",
        "            next_value = agent.get_value(next_obs).reshape(1, -1)\n",
        "            advantages = torch.zeros_like(rewards).to(device)\n",
        "            lastgaelam = 0\n",
        "            for t in reversed(range(num_steps)):\n",
        "                if t == num_steps - 1:\n",
        "                    nextnonterminal = 1.0 - next_done\n",
        "                    nextvalues = next_value\n",
        "                else:\n",
        "                    nextnonterminal = 1.0 - dones[t + 1]\n",
        "                    nextvalues = values[t + 1]\n",
        "                delta = rewards[t] + gamma * nextvalues * nextnonterminal - values[t]\n",
        "                advantages[t] = lastgaelam = (\n",
        "                    delta + gamma * gae_lambda * nextnonterminal * lastgaelam\n",
        "                )\n",
        "            returns = advantages + values\n",
        "\n",
        "        # flatten the batch\n",
        "        b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n",
        "        b_logprobs = logprobs.reshape(-1)\n",
        "        b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n",
        "        b_advantages = advantages.reshape(-1)\n",
        "        b_returns = returns.reshape(-1)\n",
        "        b_values = values.reshape(-1)\n",
        "\n",
        "        # Optimizing the policy and value network\n",
        "        b_inds = np.arange(batch_size)\n",
        "        clipfracs = []\n",
        "        for epoch in range(update_epochs):\n",
        "            np.random.shuffle(b_inds)\n",
        "            for start in range(0, batch_size, minibatch_size):\n",
        "                end = start + minibatch_size\n",
        "                mb_inds = b_inds[start:end]\n",
        "\n",
        "                _, newlogprob, entropy, newvalue = agent.get_action_and_value(\n",
        "                    b_obs[mb_inds], b_actions.long()[mb_inds]\n",
        "                )\n",
        "                logratio = newlogprob - b_logprobs[mb_inds]\n",
        "                ratio = logratio.exp()\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
        "                    old_approx_kl = (-logratio).mean()\n",
        "                    approx_kl = ((ratio - 1) - logratio).mean()\n",
        "                    clipfracs += [\n",
        "                        ((ratio - 1.0).abs() > clip_coef).float().mean().item()\n",
        "                    ]\n",
        "\n",
        "                mb_advantages = b_advantages[mb_inds]\n",
        "                if norm_adv:\n",
        "                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (\n",
        "                        mb_advantages.std() + 1e-8\n",
        "                    )\n",
        "\n",
        "                # Policy loss\n",
        "                pg_loss1 = -mb_advantages * ratio\n",
        "                pg_loss2 = -mb_advantages * torch.clamp(\n",
        "                    ratio, 1 - clip_coef, 1 + clip_coef\n",
        "                )\n",
        "                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
        "\n",
        "                # Value loss\n",
        "                newvalue = newvalue.view(-1)\n",
        "                if clip_vloss:\n",
        "                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n",
        "                    v_clipped = b_values[mb_inds] + torch.clamp(\n",
        "                        newvalue - b_values[mb_inds], -clip_coef, clip_coef\n",
        "                    )\n",
        "                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
        "                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
        "                    v_loss = 0.5 * v_loss_max.mean()\n",
        "                else:\n",
        "                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n",
        "\n",
        "                entropy_loss = entropy.mean()\n",
        "                loss = pg_loss - ent_coef * entropy_loss + v_loss * vf_coef\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
        "                optimizer.step()\n",
        "\n",
        "            if target_kl is not None and approx_kl > target_kl:\n",
        "                break\n",
        "\n",
        "        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
        "        var_y = np.var(y_true)\n",
        "        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
        "\n",
        "        # TRY NOT TO MODIFY: record rewards for plotting purposes\n",
        "        writer.add_scalar(\n",
        "            \"charts/learning_rate\", optimizer.param_groups[0][\"lr\"], global_step\n",
        "        )\n",
        "        writer.add_scalar(\"losses/value_loss\", v_loss.item(), global_step)\n",
        "        writer.add_scalar(\"losses/policy_loss\", pg_loss.item(), global_step)\n",
        "        writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n",
        "        writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl.item(), global_step)\n",
        "        writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n",
        "        writer.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), global_step)\n",
        "        writer.add_scalar(\"losses/explained_variance\", explained_var, global_step)\n",
        "        writer.add_scalar(\n",
        "            \"charts/SPS\", int(global_step / (time.time() - start_time)), global_step\n",
        "        )\n",
        "\n",
        "    envs.close()\n",
        "    writer.close()\n",
        "    if track:\n",
        "        wandb.finish()\n",
        "\n",
        "    return collected_rewards\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    games = [\n",
        "        \"Breakout-v5\",\n",
        "        \"MsPacman-v5\",\n",
        "        \"Qbert-v5\",\n",
        "        \"KungFuMaster-v5\",\n",
        "        \"Amidar-v5\",\n",
        "        \"BankHeist-v5\",\n",
        "        \"Kangaroo-v5\",\n",
        "        \"Klax-v5\",\n",
        "    ]\n",
        "    shooter_games = [\n",
        "        \"YarsRevenge-v5\",\n",
        "        \"Hero-v5\",\n",
        "        \"Krull-v5\",\n",
        "        \"Tutankham-v5\",\n",
        "        \"Assault-v5\",\n",
        "        \"Asteroids-v5\",\n",
        "        \"BeamRider-v5\",\n",
        "        \"Berzerk-v5\",\n",
        "        \"DemonAttack-v5\",\n",
        "        \"Phoenix-v5\",\n",
        "        \"SpaceInvaders-v5\",\n",
        "        \"StarGunner-v5\"\n",
        "    ]\n",
        "    optimizers = [\"adam\", \"kron\"]\n",
        "\n",
        "    for game in games:\n",
        "        for optimizer in optimizers:\n",
        "            print(f\"Running with optimizer: {optimizer}\")\n",
        "            main(\n",
        "                exp_name=f\"ppo_atari_{optimizer}\",\n",
        "                env_id=game,\n",
        "                optimizer=optimizer,\n",
        "                learning_rate=0.00025,\n",
        "                weight_decay=0.0001,  # cleanrl default was 0.0\n",
        "                total_timesteps=10000000,\n",
        "                anneal_lr=True,\n",
        "                num_envs=16,  # cleanrl default was 8\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ACUiGlX7l3hE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}